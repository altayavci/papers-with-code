{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "50996607-59f9-4d5c-a384-fcfb985ab847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/altayavci/Desktop'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc55a599-80e8-4eb0-9f0d-cfc3c6cdfd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1081d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "34363c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ae3503cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11fe39a50>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c2cfc1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-12 00:20:42--  https://raw.githubusercontent.com/hackerb9/ssa-baby-names/refs/heads/main/allnames.txt\n",
      "raw.githubusercontent.com (raw.githubusercontent.com) çözümleniyor... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "raw.githubusercontent.com (raw.githubusercontent.com)[185.199.109.133]:443 bağlanılıyor... bağlantı kuruldu.\n",
      "HTTP isteği gönderildi, yanıt bekleniyor... 200 OK\n",
      "Uzunluk: 755579 (738K) [text/plain]\n",
      "Kayıt yeri: `allnames.txt.4'\n",
      "\n",
      "allnames.txt.4      100%[===================>] 737,87K  3,91MB/s    içinde 0,2s\n",
      "\n",
      "2025-02-12 00:20:42 (3,91 MB/s) - `allnames.txt.4' kaydedildi [755579/755579]\n",
      "\n",
      "Prepended http:// to '/Users/altayavci/Desktop/allnames.txt'\n",
      "http:///Users/altayavci/Desktop/allnames.txt: Makine ismi geçersiz.\n",
      "TAMAMLANDI --2025-02-12 00:20:42--\n",
      "Toplam duvar saati zamanı: 0,6s\n",
      "İndirilen: 1 dosya, 738K, 0,2s (3,91 MB/s) içerisinde\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/hackerb9/ssa-baby-names/refs/heads/main/allnames.txt /Users/altayavci/Desktop/allnames.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0f3e6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"allnames.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "    names = [name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5193e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [' '] + sorted(list(set(''.join(names)))) + ['.']\n",
    "itoc = {i: c for i, c in enumerate(alphabet)}\n",
    "ctoi = {c: i for i, c in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7950ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda name : [ctoi[c] for c in name]\n",
    "decode = lambda tokens : ''.join([itoc[i] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f17ffc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=int(0.9*len(names))\n",
    "train_data, val_data = random_split(names, [n, len(names)-n])\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        self.ctoi = ctoi\n",
    "        self.alphabet_size = len(alphabet)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        x = [self.ctoi[c] for c in name]  # Convert characters to indices\n",
    "        y = x[1:] + [self.ctoi[' ']]  # The next character to predict (shifted version of x)\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        return x, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0d1b988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(batch):\n",
    "    max_len = max([len(x) for x, _ in batch])  # Find the max length in the batch\n",
    "    padded_x = []\n",
    "    padded_y = []\n",
    "\n",
    "    for x, y in batch:\n",
    "        padded_x.append(F.pad(x, (0, max_len - len(x)), \"constant\", ctoi[' ']))  # Pad x\n",
    "        padded_y.append(F.pad(y, (0, max_len - len(x)), \"constant\", ctoi['.']))  # Pad y\n",
    "\n",
    "    # Stack the padded sequences to create the batch\n",
    "    return torch.stack(padded_x), torch.stack(padded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ff16275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NameDataset(train_data)\n",
    "val_dataset = NameDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_sequences)\n",
    "\n",
    "name = next(iter(train_loader)) # Tuple of (x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "22188eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanalee   \n",
      "tensor([ 8,  1, 14,  1, 12,  5,  5,  0,  0,  0], device='mps:0')\n",
      "tensor([ 1, 14,  1, 12,  5,  5,  0, 27, 27, 27], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(decode(name[0].tolist()[0]))\n",
    "print(name[0][0]) # grab the 0th name\n",
    "print(name[1][0]) # grab the 0th target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5c4276aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "n_embd=len(alphabet)\n",
    "\n",
    "x=torch.tensor(encode('altay')).unsqueeze(0) # to add batch dimension\n",
    "\n",
    "xenc=F.one_hot(x, num_classes=n_embd).float()\n",
    "print(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2230e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xenc.shape torch.Size([1, 5, 28])\n",
      "xenc.transpose(-2,-1).shape torch.Size([1, 28, 5])\n",
      "xenc @ xenc.transpose(-2,-1)\n",
      " tensor([[[1., 0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [1., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"xenc.shape\",xenc.shape)\n",
    "print(\"xenc.transpose(-2,-1).shape\",xenc.transpose(-2,-1).shape)\n",
    "print(\"xenc @ xenc.transpose(-2,-1)\\n\",xenc @ xenc.transpose(-2,-1)) # (5x28) * (28x5) -> (5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "11bd870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print((xenc @ xenc.transpose(-2,-1)) @ xenc) # (1x5x5) x (1x5x28) -> (1x5x28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "36c45090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3222, 0.1185, 0.1185, 0.3222, 0.1185],\n",
       "         [0.1488, 0.4046, 0.1488, 0.1488, 0.1488],\n",
       "         [0.1488, 0.1488, 0.4046, 0.1488, 0.1488],\n",
       "         [0.3222, 0.1185, 0.1185, 0.3222, 0.1185],\n",
       "         [0.1488, 0.1488, 0.1488, 0.1488, 0.4046]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ xenc.transpose(-2,-1)).softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b0845dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.6444, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1185, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1185, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1185, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.4046, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1488, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1488, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1488, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.4046, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1488, 0.0000, 0.0000],\n",
      "         [0.0000, 0.6444, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1185, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1185, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1185, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1488, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1488, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4046, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "attn = (xenc @ xenc.transpose(-2,-1)).softmax(dim=-1) @ xenc\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c859dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode('altay') [1, 12, 20, 1, 25]\n",
      "attn.argmax(dim=-1) tensor([[ 1, 12, 20,  1, 25]])\n",
      "decode(attn.argmax(dim=-1)) altay\n"
     ]
    }
   ],
   "source": [
    "print(\"encode('altay')\",encode('altay'))\n",
    "print(\"attn.argmax(dim=-1)\",attn.argmax(dim=-1))\n",
    "print(\"decode(attn.argmax(dim=-1))\",decode(attn.argmax(dim=-1).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1650f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = xenc.shape\n",
    "dk = C\n",
    "\n",
    "query = nn.Linear(C, dk, bias=False)\n",
    "key = nn.Linear(C, dk, bias=False) \n",
    "value = nn.Linear(C, dk, bias=False) \n",
    "\n",
    "Q = query(xenc) # B x T x dk\n",
    "K = key(xenc) # B x T x dk\n",
    "V = value(xenc) # B x T x dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e3ea885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ((Q @ K.transpose(-2,-1))/(dk**0.5)).softmax(dim=-1) @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "21b20435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yyyyy'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(attn.argmax(dim=-1)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6e9f8f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kgmkk\n",
      "v.o s\n",
      "glbpn\n",
      "wvuaa\n",
      "ejirf\n",
      "hzkjz\n",
      "rqsdc\n",
      "owhay\n",
      "wbhyh\n",
      "q.fj.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):  \n",
    "  attn_probs = attn.softmax(dim=-1)  # Apply softmax to get probabilities over the vocabulary\n",
    "  sampled_indices = torch.multinomial(attn_probs.view(-1, attn_probs.size(-1)), 1)\n",
    "  print(decode(sampled_indices.T[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e1b295a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = embed_dim // num_heads\n",
    "\n",
    "        # Linear layers for query, key, and value (in the case of cross-attention, separate inputs are used)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B, T, C = q.shape  # Assuming q, k, v have the same shape (B: batch size, T: sequence length, C: embedding dim)\n",
    "\n",
    "        # Project Q, K, V using their respective linear layers\n",
    "        q = self.q_proj(q)  # Shape: (B, T, C)\n",
    "        k = self.k_proj(k)  # Shape: (B, T, C)\n",
    "        v = self.v_proj(v)  # Shape: (B, T, C)\n",
    "\n",
    "        # Reshape into (B, num_heads, T, dk)\n",
    "        q = q.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        k = k.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        v = v.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.dk ** 0.5)  # (B, heads, T, T)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = attn_weights @ v  # (B, heads, T, dk)\n",
    "\n",
    "        # Combine heads back to (B, T, C)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Final linear projection\n",
    "        return self.out_proj(attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f157af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiHeadAttention(28, 4)\n",
    "attn = m(xenc,xenc,xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "63946fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0633, -0.0449, -0.0528, -0.0069,  0.0101, -0.0150, -0.0305,\n",
       "           0.0099, -0.0229, -0.0366,  0.0186, -0.0557, -0.0105, -0.0334,\n",
       "          -0.0282,  0.0455, -0.0051,  0.0049,  0.0902, -0.0498, -0.0040,\n",
       "           0.0289, -0.0061, -0.0033, -0.0384,  0.0400, -0.0423, -0.0429],\n",
       "         [-0.0630, -0.0442, -0.0530, -0.0066,  0.0099, -0.0152, -0.0299,\n",
       "           0.0095, -0.0230, -0.0363,  0.0186, -0.0551, -0.0108, -0.0333,\n",
       "          -0.0279,  0.0458, -0.0044,  0.0054,  0.0898, -0.0498, -0.0039,\n",
       "           0.0292, -0.0061, -0.0031, -0.0377,  0.0406, -0.0425, -0.0420],\n",
       "         [-0.0635, -0.0447, -0.0528, -0.0066,  0.0102, -0.0154, -0.0304,\n",
       "           0.0095, -0.0230, -0.0361,  0.0187, -0.0553, -0.0107, -0.0333,\n",
       "          -0.0280,  0.0463, -0.0045,  0.0049,  0.0900, -0.0498, -0.0042,\n",
       "           0.0295, -0.0066, -0.0035, -0.0384,  0.0407, -0.0428, -0.0429],\n",
       "         [-0.0633, -0.0449, -0.0528, -0.0069,  0.0101, -0.0150, -0.0305,\n",
       "           0.0099, -0.0229, -0.0366,  0.0186, -0.0557, -0.0105, -0.0334,\n",
       "          -0.0282,  0.0455, -0.0051,  0.0049,  0.0902, -0.0498, -0.0040,\n",
       "           0.0289, -0.0061, -0.0033, -0.0384,  0.0400, -0.0423, -0.0429],\n",
       "         [-0.0632, -0.0448, -0.0532, -0.0069,  0.0100, -0.0155, -0.0303,\n",
       "           0.0099, -0.0235, -0.0360,  0.0183, -0.0553, -0.0108, -0.0327,\n",
       "          -0.0279,  0.0460, -0.0046,  0.0057,  0.0898, -0.0501, -0.0041,\n",
       "           0.0290, -0.0063, -0.0030, -0.0375,  0.0411, -0.0428, -0.0425]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "32a0f207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "66af05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=4*28, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, nhead)\n",
    "\n",
    "        # Feedforward layer\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),  # First fully connected layer\n",
    "            nn.ReLU(),                          # Non-linearity\n",
    "            nn.Linear(dim_feedforward, d_model)  # Second fully connected layer\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention block\n",
    "        attn_output = self.self_attention(src, src, src)\n",
    "        src = self.norm1(src + attn_output)  # Add &amp; Norm\n",
    "\n",
    "        # Feedforward block\n",
    "        ff_output = self.feedforward(src)\n",
    "        src = self.norm2(src + self.dropout(ff_output))  # Add &amp; Norm\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "52032f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(28, 4)\n",
    "output = encoder_layer(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9d1aa004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c6c249ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 12, 20,  1, 25]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f457ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.6392e-02,  4.8592e+00, -1.7832e-01, -1.1640e-01, -4.6500e-01,\n",
       "          -6.2416e-01, -4.3235e-01,  8.0909e-02, -2.0204e-01, -1.8812e-01,\n",
       "          -1.7107e-01, -2.5191e-01,  8.2323e-01, -3.7970e-01, -9.2976e-02,\n",
       "           9.2541e-02, -1.9284e-01, -1.0954e+00, -2.5514e-01, -4.1936e-01,\n",
       "           1.6585e-01,  1.2001e-01,  2.6042e-01, -7.4013e-01, -4.9338e-01,\n",
       "          -3.3369e-01,  1.9683e-01,  6.9380e-02],\n",
       "         [-1.1751e-01, -4.0006e-04, -3.2669e-03, -3.7973e-01, -3.0974e-02,\n",
       "          -1.9953e-01, -4.6604e-01, -2.3603e-01, -2.8427e-02, -7.5042e-02,\n",
       "          -1.1010e-01, -2.0972e-01,  5.1366e+00, -4.0702e-01, -9.0749e-02,\n",
       "          -1.9671e-01, -5.5031e-01, -4.6640e-01, -2.5662e-01, -1.3365e-01,\n",
       "          -3.2198e-01, -3.4788e-02, -2.5173e-02, -3.0440e-01, -1.2003e-01,\n",
       "          -1.8262e-01, -1.1008e-01, -7.9275e-02],\n",
       "         [-1.7732e-01, -3.3213e-01, -1.2561e-01, -2.4379e-01,  2.2722e-01,\n",
       "           2.3009e-01, -2.4946e-01, -2.7524e-01, -3.8295e-01, -3.2562e-01,\n",
       "          -1.7609e-01, -1.3864e-01,  1.7384e-01, -3.2441e-01,  3.6674e-01,\n",
       "           6.3823e-02, -7.8248e-01, -1.0982e+00, -3.9397e-01, -1.7696e-01,\n",
       "           4.8527e+00,  4.0390e-01,  1.2825e-02, -7.4368e-01, -7.2839e-01,\n",
       "          -4.1430e-02,  4.0116e-01, -1.5970e-02],\n",
       "         [-3.2204e-02,  4.8615e+00, -1.5242e-01, -1.1218e-01, -4.6064e-01,\n",
       "          -6.1974e-01, -4.2800e-01,  8.5050e-02, -1.9779e-01, -1.8388e-01,\n",
       "          -4.6421e-02, -2.4764e-01,  8.2708e-01, -3.7537e-01, -8.8766e-02,\n",
       "           9.6678e-02, -2.9986e-01, -1.0908e+00, -2.5087e-01, -4.1502e-01,\n",
       "           1.6996e-01,  1.2414e-01,  2.6449e-01, -7.3566e-01, -4.8901e-01,\n",
       "          -3.2938e-01,  2.0093e-01, -7.4127e-02],\n",
       "         [-5.2549e-02, -5.5657e-01,  2.1894e-01, -3.7254e-01, -1.1495e-01,\n",
       "          -5.7350e-01, -5.7672e-01,  1.3811e-01, -1.0576e-02, -9.5793e-02,\n",
       "          -9.0867e-02, -3.5312e-01,  4.3332e-01, -2.9733e-01,  2.2458e-01,\n",
       "           1.9660e-01, -4.3077e-01, -7.2360e-01, -2.4449e-01, -3.8431e-01,\n",
       "          -1.8113e-01,  4.6091e-02, -1.1188e-01, -5.0777e-01, -5.1246e-01,\n",
       "           4.9812e+00, -5.2548e-02,  4.6830e-03]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b786251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=16):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a long enough \"position\" tensor\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))  # (embed_dim / 2)\n",
    "\n",
    "        # Apply the sine and cosine functions\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "\n",
    "        # Register the positional encoding as a buffer (no gradient updates)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        return x + self.pe[:x.size(1)]  # Add the positional encoding to the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4551c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PositionalEncoding(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "00fbb9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.forward(xenc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "37d7cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  2.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  4.9510e-01,  8.6884e-01,  2.6506e-01,\n",
       "           9.6423e-01,  1.3850e-01,  9.9036e-01,  7.1906e-02,  9.9741e-01,\n",
       "           3.7267e-02,  9.9931e-01,  1.0193e+00,  9.9981e-01,  9.9998e-03,\n",
       "           9.9995e-01,  5.1795e-03,  9.9999e-01,  2.6827e-03,  1.0000e+00,\n",
       "           1.3895e-03,  1.0000e+00,  7.1969e-04,  1.0000e+00,  3.7276e-04,\n",
       "           1.0000e+00,  1.9307e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  8.6032e-01,  5.0976e-01,  5.1116e-01,\n",
       "           8.5948e-01,  2.7434e-01,  9.6163e-01,  1.4344e-01,  9.8966e-01,\n",
       "           7.4483e-02,  9.9722e-01,  3.8604e-02,  9.9925e-01,  1.9999e-02,\n",
       "           9.9980e-01,  1.0359e-02,  9.9995e-01,  5.3654e-03,  9.9999e-01,\n",
       "           1.0028e+00,  1.0000e+00,  1.4394e-03,  1.0000e+00,  7.4552e-04,\n",
       "           1.0000e+00,  3.8614e-04,  1.0000e+00],\n",
       "         [ 1.4112e-01,  1.0008e-02,  9.9986e-01,  1.6953e-02,  7.2070e-01,\n",
       "           6.9325e-01,  4.0488e-01,  9.1437e-01,  2.1423e-01,  9.7678e-01,\n",
       "           1.1159e-01,  9.9375e-01,  5.7889e-02,  9.9832e-01,  2.9995e-02,\n",
       "           9.9955e-01,  1.5538e-02,  9.9988e-01,  8.0480e-03,  9.9997e-01,\n",
       "           4.1685e-03,  9.9999e-01,  2.1591e-03,  1.0000e+00,  1.1183e-03,\n",
       "           1.0000e+00,  5.7921e-04,  1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  8.7711e-01, -4.8030e-01,  8.7867e-01,\n",
       "           4.7742e-01,  5.2762e-01,  8.4948e-01,  2.8391e-01,  9.5885e-01,\n",
       "           1.4855e-01,  9.8890e-01,  7.7151e-02,  9.9702e-01,  3.9989e-02,\n",
       "           9.9920e-01,  2.0716e-02,  9.9979e-01,  1.0731e-02,  9.9994e-01,\n",
       "           5.5580e-03,  9.9998e-01,  2.8787e-03,  1.0000e+00,  1.4910e-03,\n",
       "           2.0000e+00,  7.7228e-04,  1.0000e+00]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.forward(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "51681c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8065, -0.9420,  0.1425,  0.3147,  0.1797,  0.4637, -0.6893,\n",
       "          -0.3930,  1.8023, -0.0950, -2.7983, -0.8151,  1.1483,  0.6912,\n",
       "          -0.1588,  0.6424,  0.6942, -0.0144,  0.5377, -0.9814,  0.8732,\n",
       "           0.3436,  1.9760,  0.4262,  0.7866, -0.0064,  1.1634,  1.3267],\n",
       "         [-1.3687,  1.2084, -0.9066, -1.1218,  0.1798,  0.4476, -0.2035,\n",
       "          -1.3437,  2.1148, -0.4132, -0.1699,  0.5577, -0.3864,  0.1074,\n",
       "          -0.2942,  0.1477,  2.0658,  0.4589, -0.7438, -0.1498, -0.6487,\n",
       "          -0.8055,  0.7233,  0.3981, -0.6924, -0.7644,  0.6505, -0.3634],\n",
       "         [-0.5435,  0.6724, -0.8850,  0.2334,  0.2622, -0.7276,  0.9897,\n",
       "           0.5145, -0.3607, -0.4103, -0.3342,  0.5809, -3.0052, -0.3236,\n",
       "           2.4828,  1.0565, -0.6862, -0.7511,  0.1387, -0.6260, -0.3275,\n",
       "           0.5766,  0.6531,  0.5626, -0.1279, -0.1543,  0.4973,  0.2991],\n",
       "         [-0.5058, -0.7808, -1.3996, -1.5269,  0.5652, -1.1678, -2.3982,\n",
       "          -0.1564, -0.2024,  0.1575,  1.4432,  1.0320, -0.4041,  0.2336,\n",
       "          -1.5421, -1.1314, -2.2057, -1.3605, -0.5623, -1.8512, -1.1073,\n",
       "           0.0392, -1.7348, -0.5991,  0.0498,  0.1051, -0.4680, -0.2075],\n",
       "         [ 1.7408,  0.2563,  1.4382, -0.3349,  0.9518, -0.0742, -0.1809,\n",
       "           0.4395, -0.8161,  0.7345, -1.6091,  0.9087, -0.5828, -0.2811,\n",
       "          -1.0437, -1.7388, -1.8537, -0.7264, -0.9185, -0.9312, -1.0861,\n",
       "           0.0498,  1.0093, -0.5861,  1.2014, -0.9466,  0.7105,  0.1643]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpe = nn.Embedding(len(alphabet),28)\n",
    "\n",
    "pos = torch.arange(0, T, dtype=torch.long).unsqueeze(0) # shape (1, T)\n",
    "wpe(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "92151ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmasked attention:\n",
      " tensor([[[0.3222, 0.1185, 0.1185, 0.3222, 0.1185],\n",
      "         [0.1488, 0.4046, 0.1488, 0.1488, 0.1488],\n",
      "         [0.1488, 0.1488, 0.4046, 0.1488, 0.1488],\n",
      "         [0.3222, 0.1185, 0.1185, 0.3222, 0.1185],\n",
      "         [0.1488, 0.1488, 0.1488, 0.1488, 0.4046]]])\n",
      "\n",
      "Masked attention:\n",
      " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2689, 0.7311, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2119, 0.2119, 0.5761, 0.0000, 0.0000],\n",
      "         [0.3655, 0.1345, 0.1345, 0.3655, 0.0000],\n",
      "         [0.1488, 0.1488, 0.1488, 0.1488, 0.4046]]])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = xenc.shape\n",
    "\n",
    "print(f\"Unmasked attention:\\n {(xenc @ xenc.transpose(-2,-1)).softmax(dim=-1)}\")\n",
    "print()\n",
    "wei = xenc @ xenc.transpose(-2,-1) \n",
    "wei = wei.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1) \n",
    "\n",
    "print(f\"Masked attention:\\n {wei}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7b70eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = embed_dim // num_heads\n",
    "\n",
    "        # Linear layers for query, key, and value (in the case of cross-attention, separate inputs are used)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B, T, C = q.shape  # Assuming q, k, v have the same shape (B: batch size, T: sequence length, C: embedding dim)\n",
    "\n",
    "        # Project Q, K, V using their respective linear layers\n",
    "        q = self.q_proj(q)  # Shape: (B, T, C)\n",
    "        k = self.k_proj(k)  # Shape: (B, T, C)\n",
    "        v = self.v_proj(v)  # Shape: (B, T, C)\n",
    "\n",
    "        # Reshape into (B, num_heads, T, dk)\n",
    "        q = q.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        k = k.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "        v = v.view(B, T, self.num_heads, self.dk).transpose(1, 2)  # (B, heads, T, dk)\n",
    "\n",
    "        # Scaled dot-product attention with mask\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.dk ** 0.5)  # (B, heads, T, T)\n",
    "        mask = torch.tril(torch.ones(T, T)).to(q.device)\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = attn_weights @ v  # (B, heads, T, dk)\n",
    "\n",
    "        # Combine heads back to (B, T, C)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Final linear projection\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "78432147",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MaskedMultiHeadAttention(28,4)\n",
    "attn = m(xenc,xenc,xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "08f19519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b0979bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked Multi-Head Attention\n",
    "        self.self_attention = MaskedMultiHeadAttention(d_model, nhead)\n",
    "\n",
    "        # Feedforward layer\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),  # First fully connected layer\n",
    "            nn.ReLU(),                          # Non-linearity\n",
    "            nn.Linear(dim_feedforward, d_model)  # Second fully connected layer\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention block\n",
    "        attn_output = self.self_attention(src, src, src)\n",
    "        src = self.norm1(src + attn_output)  # Add &amp; Norm\n",
    "\n",
    "        # Feedforward block\n",
    "        ff_output = self.feedforward(src)\n",
    "        src = self.norm2(src + self.dropout(ff_output))  # Add &amp; Norm\n",
    "\n",
    "        return src\n",
    "\n",
    "encoder_layer = TransformerDecoderLayer(28, 4)\n",
    "output = encoder_layer(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1f12a7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b374c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNameGenerator(nn.Module):\n",
    "  def __init__(self, d_model, nhead, nlayers, max_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.nhead = nhead\n",
    "    self.embed = nn.Embedding(len(alphabet), d_model)\n",
    "    #self.pe = PositionalEncoding(d_model)\n",
    "    self.wpe = nn.Embedding(max_length,d_model)\n",
    "    self.decoder = nn.ModuleList([TransformerDecoderLayer(d_model, nhead) for _ in range(nlayers)])\n",
    "\n",
    "    self.linear = nn.Linear(d_model, len(alphabet))\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T = x.size()\n",
    "    print(x)\n",
    "    x = self.embed(x)\n",
    "\n",
    "    #x = self.pe(x)\n",
    "    pos = torch.arange(0, T, dtype=torch.long, device=x.device).unsqueeze(0) # shape (1, t)\n",
    "    x = x + self.wpe(pos)\n",
    "\n",
    "    for layer in self.decoder:\n",
    "      x = layer(x)\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self, x, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits = self(x)\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = self.softmax(logits)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      if next_token == ctoi[' ']:\n",
    "        break\n",
    "      x = torch.cat((x, next_token), dim=1)\n",
    "    return x[:,1:] # drop the first seed character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "04ed094b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='mps:0')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0]).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4cc52e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], device='mps:0')\n",
      "tensor([[ 0, 15]], device='mps:0')\n",
      "tensor([[ 0, 15, 25]], device='mps:0')\n",
      "tensor([[ 0, 15, 25, 13]], device='mps:0')\n",
      "tensor([[ 0, 15, 25, 13, 10]], device='mps:0')\n",
      "tensor([[ 0, 15, 25, 13, 10,  3]], device='mps:0')\n",
      "tensor([[ 0, 15, 25, 13, 10,  3, 17]], device='mps:0')\n",
      "tensor([[ 0, 15, 25, 13, 10,  3, 17,  4]], device='mps:0')\n",
      "oymjcqdo\n",
      "Model Parameters: 277084\n"
     ]
    }
   ],
   "source": [
    "m = RandomNameGenerator(32, 4,2,16).to(device)\n",
    "\n",
    "print(decode(m.generate(torch.tensor([0]).unsqueeze(0).to(device),8).tolist()[0]))\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fc0d7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13,  1, 12,  1, 11,  0,  0,  0,  0,  0],\n",
      "        [12,  9, 12,  9, 15, 14, 14,  1,  0,  0],\n",
      "        [19, 20,  5, 16,  5, 14,  0,  0,  0,  0],\n",
      "        [20,  1, 11,  5, 12,  9,  1,  0,  0,  0],\n",
      "        [ 7,  5, 14, 14,  9,  5, 22,  5,  0,  0],\n",
      "        [20,  1, 25, 19,  8,  1, 23, 14,  1,  0],\n",
      "        [ 3,  1, 19,  9,  1, 14,  1,  0,  0,  0],\n",
      "        [10,  1, 19, 13,  5,  8,  0,  0,  0,  0],\n",
      "        [ 2, 18,  5,  1, 26,  1,  5,  0,  0,  0],\n",
      "        [19, 15,  8,  1, 14,  9,  0,  0,  0,  0],\n",
      "        [ 5, 19,  1,  2,  5, 12,  0,  0,  0,  0],\n",
      "        [ 4,  1, 11, 11, 15, 20,  1,  0,  0,  0],\n",
      "        [12, 15, 18, 18,  1, 25, 14,  5,  0,  0],\n",
      "        [11,  5, 14, 14,  5,  4,  9,  0,  0,  0],\n",
      "        [ 5, 12,  1,  9, 19,  8,  1,  0,  0,  0],\n",
      "        [26,  8,  1, 14,  5, 12,  0,  0,  0,  0],\n",
      "        [14,  9,  1, 13,  1,  0,  0,  0,  0,  0],\n",
      "        [ 1,  8, 19,  5,  5, 13,  0,  0,  0,  0],\n",
      "        [13,  1, 18,  9, 15, 19,  0,  0,  0,  0],\n",
      "        [14,  1,  4,  9,  5,  0,  0,  0,  0,  0],\n",
      "        [24, 25, 18, 21, 19,  0,  0,  0,  0,  0],\n",
      "        [ 3,  8,  5, 12, 19,  5,  1, 25,  0,  0],\n",
      "        [13, 21, 14,  4,  9,  0,  0,  0,  0,  0],\n",
      "        [ 2,  5, 25,  1, 14, 11,  1,  0,  0,  0],\n",
      "        [13, 15, 23,  7, 12,  9,  0,  0,  0,  0],\n",
      "        [ 8,  1,  9,  4,  5, 14,  0,  0,  0,  0],\n",
      "        [ 1, 12,  4,  1, 25, 19,  8,  5,  9, 14],\n",
      "        [ 5, 13, 13,  1, 10, 15, 25,  0,  0,  0],\n",
      "        [11,  1, 19,  9, 14,  0,  0,  0,  0,  0],\n",
      "        [14,  1, 20,  1, 14,  5, 13,  0,  0,  0],\n",
      "        [ 1, 14,  1,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 21, 16,  1, 12,  0,  0,  0,  0,  0]], device='mps:0')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=5e-4, weight_decay=0.01, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "for epoch in range(10):\n",
    "  for xenc_batch, y_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = m(xenc_batch)\n",
    "    logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * max_seq_len, vocab_size]\n",
    "    y_batch = y_batch.view(-1)  # Shape: [batch_size * max_seq_len]\n",
    "\n",
    "    # Compute the loss using CrossEntropyLoss\n",
    "    loss = F.cross_entropy(logits, y_batch, ignore_index=ctoi['.'])\n",
    "\n",
    "    # Backward pass\n",
    "    m.zero_grad(set_to_none=True) # make sure ALL the gradients are set to zero\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65292bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, dataset, batch_size=50, max_batches=None):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=pad_sequences)\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        X, Y = batch\n",
    "        logits = model(X)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        Y = Y.view(-1)  # Shape: [batch_size * max_seq_len]\n",
    "\n",
    "        # Compute the loss using CrossEntropyLoss\n",
    "        loss = F.cross_entropy(logits, Y, ignore_index=ctoi['.'])\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if max_batches is not None and i >= max_batches:\n",
    "            break\n",
    "    mean_loss = torch.tensor(losses).mean().item()\n",
    "    model.train() # reset model back to training mode\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = evaluate(m, train_dataset, batch_size=100, max_batches=10)\n",
    "test_loss  = evaluate(m, val_dataset,  batch_size=100, max_batches=10)\n",
    "\n",
    "print(f\"Epoch {epoch}, Train-Loss: {train_loss} Val-Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/explaining-the-attention-mechanism-29a0e7b448a9/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
